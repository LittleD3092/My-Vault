演講分為三個部份：生成式AI的原理、人類如何善用語言模型與如何教模型新技能。其實生成式AI就是在做文字接龍，它會看前面的語句與自己先前的回答產生下一個字，並且再把結果放到模型中產生下一個字。人類在善用模型時可以用給前提、範例，或者讓模型檢查自己的錯誤來提高準確率，現在常常也會搭配其他工具或先搜尋網路來提高回答的品質。在未來，不同的AI模型會互相合作，因此了解哪些AI應該做什麼工作也是很重要的。講者在教語言模型新技能時，主要遇到了在Fine tune之後語言模型在原本的工作上的正確率顯著降低，稱為Catastrophic Forgetting Issue。講者有三個解決方法。首先是Experience Replay，就是使用約5%先前訓練的資料，讓模型記得先前訓練的結果，但是我們不一定可以得到模型先前訓練的資料。第二個是Chat Vector，基本原理是將兩個模型的向量相減，這個向量可以加在我們的模型上，得到兩個模型之間相差訓練的成果。最後是讓原始模型自己輸出訓練資料，並且將這個訓練資料應用在第一種方法。

我對人工智慧的領域沒有很熟悉，所以今天聽到了講者講的一些概念感到很震撼。譬如講者有提到使用GPT模型改作業的經驗，我才知道要將模型應用在生活中可能會需要套很多層才能確保準確性。對於多個模型合作達成工作我也覺得很震撼，我本來認為模型很聰明，可以解決所有可能的問題，但實際上有許多模型分別做不同工作其實更能解決問題。