# -*- coding: utf-8 -*-
"""
Configuration for the simulations, for the multi-players case.
"""
from __future__ import division, print_function  # Python 2 compatibility

__author__ = "Lilian Besson"
__version__ = "0.9"

# Tries to know number of CPU
try:
    from multiprocessing import cpu_count
    CPU_COUNT = cpu_count()
except ImportError:
    CPU_COUNT = 1

from os import getenv
import numpy as np

if __name__ == '__main__':
    print("Warning: this script 'configuration_multiplayers.py' is NOT executable. Use 'main_multiplayers.py' or 'make multiplayers' or 'make moremultiplayers' ...")  # DEBUG
    exit(0)

try:
    # Import arms
    from Arms import *
    # Import contained classes
    from Environment import MAB
    # Collision Models
    from Environment.CollisionModels import *
    # Import algorithms, both single-player and multi-player
    from Policies import *
    from PoliciesMultiPlayers import *
except ImportError:
    from SMPyBandits.Arms import *
    from SMPyBandits.Environment import MAB
    from SMPyBandits.Environment.CollisionModels import *
    from SMPyBandits.Policies import *
    from SMPyBandits.PoliciesMultiPlayers import *


#: HORIZON : number of time steps of the experiments.
#: Warning Should be >= 10000 to be interesting "asymptotically".
HORIZON = 100
HORIZON = 500
HORIZON = 2000
HORIZON = 3000
HORIZON = 5000
HORIZON = 10000
HORIZON = int(getenv('T', HORIZON))

#: REPETITIONS : number of repetitions of the experiments.
#: Warning: Should be >= 10 to be statistically trustworthy.
REPETITIONS = 1  # XXX To profile the code, turn down parallel computing
REPETITIONS = 4  # Nb of cores, to h    ave exactly one repetition process by cores
REPETITIONS = 200
REPETITIONS = int(getenv('N', REPETITIONS))

#: To profile the code, turn down parallel computing
DO_PARALLEL = False  # XXX do not let this = False  # To profile the code, turn down parallel computing
DO_PARALLEL = True
DO_PARALLEL = (REPETITIONS > 1) and DO_PARALLEL

#: Number of jobs to use for the parallel computations. -1 means all the CPU cores, 1 means no parallelization.
N_JOBS = -1 if DO_PARALLEL else 1
if CPU_COUNT > 4:  # We are on a server, let's be nice and not use all cores
    N_JOBS = min(CPU_COUNT, max(int(CPU_COUNT / 3), CPU_COUNT - 8))
N_JOBS = int(getenv('N_JOBS', N_JOBS))

#: NB_PLAYERS : number of players for the game. Should be >= 2 and <= number of arms.
NB_PLAYERS = 1    # Less that the number of arms
NB_PLAYERS = 2    # Less that the number of arms
NB_PLAYERS = 3    # Less that the number of arms
# NB_PLAYERS = 6    # Less that the number of arms
# NB_PLAYERS = 25   # XXX More than the number of arms !!
# NB_PLAYERS = 30   # XXX More than the number of arms !!
NB_PLAYERS = int(getenv('M', NB_PLAYERS))
NB_PLAYERS = int(getenv('NB_PLAYERS', NB_PLAYERS))

# #: Different Collision models
# collisionModel = noCollision  #: Like single player.
# collisionModel = rewardIsSharedUniformly  #: Weird collision model.

# # Based on a distance of each user with the base station: the closer one wins if collision
# distances = 'uniform'  # Uniformly spaced objects
# distances = 'random'  # Let it compute the random distances, ONCE by thread, and then cache it? XXX
# distances = np.random.random_sample(NB_PLAYERS)  # Distance between 0 and 1, randomly affected!
# print("Each player is at the base station with a certain distance (the lower, the more chance it has to be selected)")  # DEBUG
# for i in range(NB_PLAYERS):
#     print("  - Player nb #{}\tis at distance {:.3g} to the Base Station ...".format(i + 1, distances[i]))  # DEBUG


# def onlyCloserUserGetsReward(t, arms, players, choices, rewards, pulls, collisions, distances=distances):
#     return closerUserGetsReward(t, arms, players, choices, rewards, pulls, collisions, distances=distances)


# collisionModel = onlyCloserUserGetsReward
# collisionModel.__doc__ = closerUserGetsReward.__doc__

#: The best collision model: none of the colliding users get any reward
collisionModel = onlyUniqUserGetsReward    # XXX this is the best one

# collisionModel = allGetRewardsAndUseCollision  #: DONE this is a bad collision model


# Parameters for the arms
VARIANCE = 0.05   #: Variance of Gaussian arms


#: Should we cache rewards? The random rewards will be the same for all the REPETITIONS simulations for each algorithms.
CACHE_REWARDS = False  # XXX to disable manually this feature


#: Number of arms for non-hard-coded problems (Bayesian problems)
NB_ARMS = 2 * NB_PLAYERS
NB_ARMS = int(getenv('K', NB_ARMS))
NB_ARMS = int(getenv('NB_ARMS', NB_ARMS))

#: Default value for the lower value of means
LOWER = 0.
#: Default value for the amplitude value of means
AMPLITUDE = 1.

#: Type of arms for non-hard-coded problems (Bayesian problems)
ARM_TYPE = "Bernoulli"
ARM_TYPE = str(getenv('ARM_TYPE', ARM_TYPE))
if ARM_TYPE in ["UnboundedGaussian"]:
    LOWER = -5
    AMPLITUDE = 10

LOWER = float(getenv('LOWER', LOWER))
AMPLITUDE = float(getenv('AMPLITUDE', AMPLITUDE))
assert AMPLITUDE > 0, "Error: invalid amplitude = {:.3g} but has to be > 0."  # DEBUG

ARM_TYPE_str = str(ARM_TYPE)
ARM_TYPE = mapping_ARM_TYPE[ARM_TYPE]

#: True to use bayesian problem
ENVIRONMENT_BAYESIAN = False
ENVIRONMENT_BAYESIAN = getenv('BAYES', str(ENVIRONMENT_BAYESIAN)) == 'True'

#: Means of arms for non-hard-coded problems (non Bayesian)
MEANS = uniformMeans(nbArms=NB_ARMS, delta=0.05, lower=LOWER, amplitude=AMPLITUDE, isSorted=True)
MEANS = uniformMeans(nbArms=NB_ARMS, delta=0.1, lower=LOWER, amplitude=AMPLITUDE, isSorted=True)

import numpy as np
# more parametric? Read from cli?
MEANS_STR = getenv('MEANS', '')
if MEANS_STR != '':
    MEANS = [ float(m) for m in MEANS_STR.replace('[', '').replace(']', '').split(',') ]
    print("Using cli env variable to use MEANS = {}.".format(MEANS))  # DEBUG


#: This dictionary configures the experiments
configuration = {
    # --- Duration of the experiment
    "horizon": HORIZON,
    # --- Number of repetition of the experiment (to have an average)
    "repetitions": REPETITIONS,
    # --- Parameters for the use of joblib.Parallel
    "n_jobs": N_JOBS,    # = nb of CPU cores
    "verbosity": 6,      # Max joblib verbosity
    # --- Collision model
    "collisionModel": collisionModel,
    # --- Other parameters for the Evaluator
    "finalRanksOnAverage": True,  # Use an average instead of the last value for the final ranking of the tested players
    "averageOn": 1e-3,  # Average the final rank on the 1.% last time steps
    # --- Should we plot the lower-bounds or not?
    "plot_lowerbounds": True,  # XXX Default
    # --- Arms
    "environment": [
        # {   # A damn simple problem: 2 arms, one bad, one good
        #     "arm_type": Bernoulli,
        #     "params": [0.1, 0.9]  # uniformMeans(2, 0.1)
        #     # "params": [0.9, 0.9]
        #     # "params": [0.85, 0.9]
        # }
        # {   # XXX to test with 1 suboptimal arm only
        #     "arm_type": Bernoulli,
        #     "params": uniformMeans((NB_PLAYERS + 1), 1 / (1. + (NB_PLAYERS + 1)))
        # }
        # {   # XXX to test with half very bad arms, half perfect arms
        #     "arm_type": Bernoulli,
        #     "params": shuffled([0] * NB_PLAYERS) + ([1] * NB_PLAYERS)
        # }
        # {   # XXX To only test the orthogonalization (collision avoidance) protocol
        #     "arm_type": Bernoulli,
        #     "params": [1] * NB_PLAYERS
        # }
        # {   # XXX To only test the orthogonalization (collision avoidance) protocol
        #     "arm_type": Bernoulli,
        #     "params": [1] * NB_ARMS
        # }
        # {   # XXX To only test the orthogonalization (collision avoidance) protocol
        #     "arm_type": Bernoulli,
        #     "params": ([0] * (NB_ARMS - NB_PLAYERS)) + ([1] * NB_PLAYERS)
        # }
        # {   # An easy problem, but with a LOT of arms! (50 arms)
        #     "arm_type": Bernoulli,
        #     "params": uniformMeans(50, 1 / (1. + 50))
        # }
        # # XXX Default!
        # {   # A very easy problem (X arms), but it is used in a lot of articles
        #     "arm_type": ARM_TYPE,
        #     "params": uniformMeans(NB_ARMS, 1 / (1. + NB_ARMS))
        # },
        {   # Use vector from command line
            "arm_type": ARM_TYPE,
            "params": MEANS
        },
    ],
}

if ENVIRONMENT_BAYESIAN:
    configuration["environment"] = [  # XXX Bernoulli arms
        {   # A Bayesian problem: every repetition use a different mean vectors!
            "arm_type": ARM_TYPE,
            "params": {
                "function": randomMeans,
                "args": {
                    "nbArms": NB_ARMS,
                    # "mingap": None,
                    # "mingap": 0.0000001,
                    # "mingap": 0.1,
                    "mingap": 1. / (3 * NB_ARMS),
                    "lower": LOWER,
                    "amplitude": AMPLITUDE,
                    "isSorted": True,
                }
            }
        },
    ]


try:
    #: Number of arms *in the first environment*
    nbArms = int(configuration['environment'][0]['params']['args']['nbArms'])
except (TypeError, KeyError):
    nbArms = len(configuration['environment'][0]['params'])

if len(configuration['environment']) > 1:
    print("WARNING do not use this hack if you try to use more than one environment.")

#: Compute the gap of the first problem.
#: (for d in MEGA's parameters, and epsilon for MusicalChair's parameters)
try:
    GAP = np.min(np.diff(np.sort(configuration['environment'][0]['params'])))
except (ValueError, np.AxisError):
    print("Warning: using the default value for the GAP (Bayesian environment maybe?)")  # DEBUG
    GAP = 1. / (3 * NB_ARMS)


configuration["successive_players"] = [
    # # DONE test this new SIC_MMAB algorithm
    # # [ SIC_MMAB(nbArms, HORIZON) for _ in range(NB_PLAYERS) ],
    # # [ SIC_MMAB_UCB(nbArms, HORIZON) for _ in range(NB_PLAYERS) ],
    [ SIC_MMAB_klUCB(nbArms, HORIZON) for _ in range(NB_PLAYERS) ],

    # # XXX stupid version with fixed T0 : cannot adapt to any problem
    [ TrekkingTSN(nbArms, theta=0.1, epsilon=0.1, delta=0.1) for _ in range(NB_PLAYERS) ],
    # # DONE test this new TrekkingTSN algorithm!

    # # ---- rhoRand etc
    # # rhoRand(NB_PLAYERS, nbArms, UCB).children,
    # rhoRand(NB_PLAYERS, nbArms, klUCB).children,
    # # rhoRand(NB_PLAYERS, nbArms, EmpiricalMeans).children,
    # # rhoRand(NB_PLAYERS, nbArms, BESA).children,
    # # # [ Aggregator(nbArms, children=[  # XXX Not efficient!
    # # #         lambda: rhoRand(1 + x, nbArms, klUCB).children[0]
    # # #         for x in range(NB_ARMS)
    # # #         # for x in set.intersection(set(range(NB_ARMS)), [NB_PLAYERS - 1, NB_PLAYERS, NB_PLAYERS + 1])
    # # #     ]) for _ in range(NB_PLAYERS)
    # # # ],
    # # # EstimateM(NB_PLAYERS, nbArms, rhoRand, klUCB).children,
    # rhoEst(NB_PLAYERS, nbArms, klUCB).children,  # = EstimateM(... rhoRand, klUCB)
    # # # rhoEst(NB_PLAYERS, nbArms, BESA).children,  # = EstimateM(... rhoRand, klUCB)
    # # # rhoEst(NB_PLAYERS, nbArms, klUCB, threshold=threshold_on_t).children,  # = EstimateM(... rhoRand, klUCB)
    # # EstimateM(NB_PLAYERS, nbArms, rhoRand, klUCB, horizon=HORIZON, threshold=threshold_on_t_with_horizon).children,  # = rhoEstPlus(...)
    # rhoEstPlus(NB_PLAYERS, nbArms, klUCB, HORIZON).children,
    # rhoLearn(NB_PLAYERS, nbArms, klUCB, BayesUCB).children,
    # rhoLearn(NB_PLAYERS, nbArms, klUCB, klUCB).children,
    # rhoLearnExp3(NB_PLAYERS, nbArms, klUCB, feedback_function=binary_feedback, rankSelectionAlgo=Exp3Decreasing).children,
    # # rhoLearnExp3(NB_PLAYERS, nbArms, klUCB, feedback_function=ternary_feedback, rankSelectionAlgo=Exp3Decreasing).children,

    # # # ---- RandTopM
    # # RandTopM(NB_PLAYERS, nbArms, UCB).children,
    # RandTopM(NB_PLAYERS, nbArms, klUCB).children,
    # # RandTopM(NB_PLAYERS, nbArms, EmpiricalMeans).children,
    # # RandTopM(NB_PLAYERS, nbArms, BESA).children,
    # # # RandTopMCautious(NB_PLAYERS, nbArms, klUCB).children,
    # # # RandTopMExtraCautious(NB_PLAYERS, nbArms, klUCB).children,
    # # # RandTopMOld(NB_PLAYERS, nbArms, klUCB).children,
    # # # [ Aggregator(nbArms, children=[  # XXX Not efficient!
    # # #         lambda: RandTopM(1 + x, nbArms, klUCB).children[0]
    # # #         for x in range(NB_ARMS)
    # # #         # for x in set.intersection(set(range(NB_ARMS)), [NB_PLAYERS - 1, NB_PLAYERS, NB_PLAYERS + 1])
    # # #     ]) for _ in range(NB_PLAYERS)
    # # # ],
    # # EstimateM(NB_PLAYERS, nbArms, RandTopM, klUCB).children,  # FIXME experimental!
    # RandTopMEst(NB_PLAYERS, nbArms, klUCB).children,  # = EstimateM(... RandTopM, klUCB)
    # RandTopMEstPlus(NB_PLAYERS, nbArms, klUCB, HORIZON).children,  # FIXME experimental!

    # # ---- MCTopM
    # # MCTopM(NB_PLAYERS, nbArms, UCB).children,
    # MCTopM(NB_PLAYERS, nbArms, klUCB).children,
    # # MCTopM(NB_PLAYERS, nbArms, EmpiricalMeans).children,
    # # MCTopM(NB_PLAYERS, nbArms, BESA).children,
    # # MCTopMCautious(NB_PLAYERS, nbArms, klUCB).children,
    # # MCTopMExtraCautious(NB_PLAYERS, nbArms, klUCB).children,
    # # MCTopMOld(NB_PLAYERS, nbArms, klUCB).children,
    # # [ Aggregator(nbArms, children=[  # XXX Not efficient!
    # #         lambda: MCTopM(1 + x, nbArms, klUCB).children[0]
    # #         for x in range(NB_ARMS)
    # #         # for x in set.intersection(set(range(NB_ARMS)), [NB_PLAYERS - 1, NB_PLAYERS, NB_PLAYERS + 1])
    # #     ]) for _ in range(NB_PLAYERS)
    # # ],
    # # EstimateM(NB_PLAYERS, nbArms, MCTopM, klUCB).children,  # FIXME experimental!
    # MCTopMEst(NB_PLAYERS, nbArms, klUCB).children,  # = EstimateM(... MCTopM, klUCB)
    # # # MCTopMEst(NB_PLAYERS, nbArms, BESA).children,  # = EstimateM(... MCTopM, klUCB)
    # MCTopMEstPlus(NB_PLAYERS, nbArms, klUCB, HORIZON).children,  # FIXME experimental!
    # # MCTopMEstPlus(NB_PLAYERS, nbArms, BESA, HORIZON).children,  # FIXME experimental!

    # # # ---- Selfish
    # # # Selfish(NB_PLAYERS, nbArms, Exp3Decreasing).children,
    # # # Selfish(NB_PLAYERS, nbArms, Exp3PlusPlus).children,
    # Selfish(NB_PLAYERS, nbArms, UCB).children,
    Selfish(NB_PLAYERS, nbArms, klUCB).children,
    # Selfish(NB_PLAYERS, nbArms, EmpiricalMeans).children,
    # # Selfish(NB_PLAYERS, nbArms, Aggregator, children=[UCB, klUCB, EmpiricalMeans]).children,
    # # # Selfish(NB_PLAYERS, nbArms, BESA).children,
    # # # [ Aggregator(nbArms, children=[Exp3Decreasing, Exp3PlusPlus, UCB, MOSS, klUCB, BayesUCB, Thompson, DMEDPlus]) for _ in range(NB_PLAYERS) ],  # exactly like Selfish(NB_PLAYERS, nbArms, Aggregator, children=[...])
    # # [ Aggregator(nbArms, children=[UCB, klUCB, Thompson]) for _ in range(NB_PLAYERS) ],  # exactly like Selfish(NB_PLAYERS, nbArms, Aggregator, children=[...])

    # # --- 22) Comparing Selfish, rhoRand, rhoLearn, RandTopM for klUCB, and estimating M
    # # CentralizedMultiplePlay(NB_PLAYERS, nbArms, EmpiricalMeans).children,
    # # CentralizedMultiplePlay(NB_PLAYERS, nbArms, Exp3Decreasing).children,
    # # CentralizedMultiplePlay(NB_PLAYERS, nbArms, Exp3PlusPlus).children,
    # CentralizedMultiplePlay(NB_PLAYERS, nbArms, UCB).children,
    CentralizedMultiplePlay(NB_PLAYERS, nbArms, klUCB).children,
    # # CentralizedMultiplePlay(NB_PLAYERS, nbArms, BESA).children,
    # # CentralizedMultiplePlay(NB_PLAYERS, nbArms, Aggregator, children=[UCB, MOSS, klUCB, BayesUCB, Thompson, DMEDPlus]).children,  # XXX don't work so well

    # # FIXME how to chose the 5 parameters for MEGA policy ?
    # # XXX By trial and error??
    # # d should be smaller than the gap Delta = mu_M* - mu_(M-1)* (gap between Mbest and Mworst)
    # [ MEGA(nbArms, p0=0.1, alpha=0.1, beta=0.5, c=0.1, d=0.99*GAP) for _ in range(NB_PLAYERS) ],  # XXX always linear regret!

    # # # XXX stupid version with fixed T0 : cannot adapt to any problem
    [ MusicalChair(nbArms, Time0=1000) for _ in range(NB_PLAYERS) ],
    [ MusicalChair(nbArms, Time0=50*NB_ARMS) for _ in range(NB_PLAYERS) ],
    [ MusicalChair(nbArms, Time0=100*NB_ARMS) for _ in range(NB_PLAYERS) ],
    [ MusicalChair(nbArms, Time0=150*NB_ARMS) for _ in range(NB_PLAYERS) ],
    # # # XXX cheated version, with known gap (epsilon < Delta) and proba of success 5% !
    [ MusicalChair(nbArms, Time0=optimalT0(nbArms=NB_ARMS, epsilon=0.99*GAP, delta=0.5)) for _ in range(NB_PLAYERS) ],
    [ MusicalChair(nbArms, Time0=optimalT0(nbArms=NB_ARMS, epsilon=0.99*GAP, delta=0.1)) for _ in range(NB_PLAYERS) ],
    # # XXX cheated version, with known gap and known horizon (proba of success delta < 1 / T) !
    [ MusicalChair(nbArms, Time0=optimalT0(nbArms=NB_ARMS, epsilon=0.99*GAP, delta=1./(1+HORIZON))) for _ in range(NB_PLAYERS) ],

    # # FIXME an extension of the MusicalChair for the NoSensing case
    [ MusicalChairNoSensing(nbPlayers=NB_PLAYERS, nbArms=nbArms, horizon=HORIZON, constant_c=1) for _ in range(NB_PLAYERS) ],
    [ MusicalChairNoSensing(nbPlayers=NB_PLAYERS, nbArms=nbArms, horizon=HORIZON, constant_c=10) for _ in range(NB_PLAYERS) ],
    [ MusicalChairNoSensing(nbPlayers=NB_PLAYERS, nbArms=nbArms, horizon=HORIZON, constant_c=128) for _ in range(NB_PLAYERS) ],
]

# XXX Comparing different rhoRand approaches
# configuration["successive_players"] = [
#     rhoRand(NB_PLAYERS, nbArms, UCBalpha, alpha=1).children,  # This one is efficient!
#     rhoRand(NB_PLAYERS, nbArms, UCBalpha, alpha=0.25).children,  # This one is efficient!
#     rhoRand(NB_PLAYERS, nbArms, MOSS).children,
#     rhoRand(NB_PLAYERS, nbArms, klUCB).children,
#     rhoRand(NB_PLAYERS, nbArms, klUCBPlus).children,
#     rhoRand(NB_PLAYERS, nbArms, Thompson).children,
#     rhoRand(NB_PLAYERS, nbArms, SoftmaxDecreasing).children,
#     rhoRand(NB_PLAYERS, nbArms, BayesUCB).children,
#     rhoRand(NB_PLAYERS, nbArms, AdBandits, alpha=0.5, horizon=HORIZON).children,
# ]


# XXX Comparing different ALOHA approaches
# from itertools import product  # XXX If needed!
# p0 = 1. / NB_PLAYERS
# p0 = 0.75
# configuration["successive_players"] = [
#     Selfish(NB_PLAYERS, nbArms, BayesUCB).children,  # This one is efficient!
# ] + [
#     ALOHA(NB_PLAYERS, nbArms, BayesUCB, p0=p0, alpha_p0=alpha_p0, beta=beta).children
#     # ALOHA(NB_PLAYERS, nbArms, BayesUCB, p0=p0, alpha_p0=alpha_p0, ftnext=tnext_log).children,
#     for alpha_p0, beta in product([0.05, 0.25, 0.5, 0.75, 0.95], repeat=2)
#     # for alpha_p0, beta in product([0.1, 0.5, 0.9], repeat=2)
# ]

# # XXX Comparing different centralized approaches
# configuration["successive_players"] = [
#     CentralizedMultiplePlay(NB_PLAYERS, nbArms, UCBalpha).children,
#     CentralizedIMP(NB_PLAYERS, nbArms, UCBalpha).children,
#     CentralizedMultiplePlay(NB_PLAYERS, nbArms, Thompson).children,
#     CentralizedIMP(NB_PLAYERS, nbArms, Thompson).children,
#     CentralizedMultiplePlay(NB_PLAYERS, nbArms, klUCBPlus).children,
# ]

# XXX Comparing different UCB vs klUCB approaches
configuration["successive_players"] = [
    CentralizedMultiplePlay(NB_PLAYERS, nbArms, UCB).children,
    CentralizedMultiplePlay(NB_PLAYERS, nbArms, klUCB).children,
    Selfish(NB_PLAYERS, nbArms, UCB).children,
    Selfish(NB_PLAYERS, nbArms, klUCB).children,
    rhoRand(NB_PLAYERS, nbArms, UCB).children,
    rhoRand(NB_PLAYERS, nbArms, klUCB).children,
    RandTopM(NB_PLAYERS, nbArms, UCB).children,
    RandTopM(NB_PLAYERS, nbArms, klUCB).children,
    MCTopM(NB_PLAYERS, nbArms, UCB).children,
    MCTopM(NB_PLAYERS, nbArms, klUCB).children,
]


configuration.update({
    # --- DONE Defining manually each child
    # "players": [TakeFixedArm(nbArms, nbArms - 1) for _ in range(NB_PLAYERS)]
    # "players": [TakeRandomFixedArm(nbArms) for _ in range(NB_PLAYERS)]

    # --- Defining each player as one child of a multi-player policy

    # --- DONE Using multi-player dummy Centralized policy
    # XXX each player needs to now the number of players
    # "players": CentralizedFixed(NB_PLAYERS, nbArms).children
    # "players": CentralizedCycling(NB_PLAYERS, nbArms).children
    # --- DONE Using a smart Centralized policy, based on choiceMultiple()
    # "players": CentralizedMultiplePlay(NB_PLAYERS, nbArms, UCB, uniformAllocation=False).children
    # "players": CentralizedMultiplePlay(NB_PLAYERS, nbArms, UCB, uniformAllocation=True).children
    # "players": CentralizedMultiplePlay(NB_PLAYERS, nbArms, Thompson, uniformAllocation=False).children
    # "players": CentralizedMultiplePlay(NB_PLAYERS, nbArms, Thompson, uniformAllocation=True).children

    # --- DONE Using a smart Centralized policy, based on choiceIMP() -- It's not better, in fact
    # "players": CentralizedIMP(NB_PLAYERS, nbArms, UCB, uniformAllocation=False).children
    # "players": CentralizedIMP(NB_PLAYERS, nbArms, UCB, uniformAllocation=True).children
    # "players": CentralizedIMP(NB_PLAYERS, nbArms, Thompson, uniformAllocation=False).children
    # "players": CentralizedIMP(NB_PLAYERS, nbArms, Thompson, uniformAllocation=True).children

    # --- DONE Using multi-player Selfish policy
    # "players": Selfish(NB_PLAYERS, nbArms, Uniform).children
    # "players": Selfish(NB_PLAYERS, nbArms, TakeRandomFixedArm).children
    # "players": Selfish(NB_PLAYERS, nbArms, Exp3Decreasing).children
    # "players": Selfish(NB_PLAYERS, nbArms, Exp3WithHorizon, horizon=HORIZON).children
    # "players": Selfish(NB_PLAYERS, nbArms, UCB).children

    # --- TODO play with SIC_MMAB
    "players": [ SIC_MMAB(nbArms, HORIZON) for _ in range(NB_PLAYERS) ]
})
# TODO the EvaluatorMultiPlayers should regenerate the list of players in every repetitions, to have at the end results on the average behavior of these randomized multi-players policies


# DONE
print("Loaded experiments configuration from 'configuration_multiplayers.py' :")
print("configuration =", configuration)  # DEBUG
